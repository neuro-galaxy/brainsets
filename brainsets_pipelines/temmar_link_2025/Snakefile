######################################################
# Temmar et al. (2025)
# LINK: Long-term Intracortical Neural activity and Kinematics
#
# 312 sessions from Monkey N over 3.5 years
# 96-channel threshold crossings + 2-finger kinematics
######################################################

DATASET = "temmar_link_2025"
DANDI_ID = "001201"

RAW_DIR = config.get("RAW_DIR", "../LINK_dataset/data")
PROCESSED_DIR = config.get("PROCESSED_DIR", "./processed")

checkpoint temmar_link_download_data:
    output:
        f"{RAW_DIR}/{DATASET}/manifest.txt"
    run:
        import os
        from pathlib import Path
        import subprocess

        # Create directory (cross-platform)
        output_dir = Path(f"{RAW_DIR}/{DATASET}")
        output_dir.mkdir(parents=True, exist_ok=True)

        # Download data
        subprocess.run(
            f"dandi download --existing skip -o {RAW_DIR}/{DATASET} DANDI:{DANDI_ID}",
            shell=True, check=True
        )

        # Find NWB files and create manifest (cross-platform Python)
        nwb_files = []
        for nwb_file in output_dir.rglob("*.nwb"):
            rel_path = nwb_file.relative_to(output_dir)
            nwb_files.append(str(rel_path).replace("\\", "/"))

        # Write manifest
        with open(output[0], 'w') as f:
            for file_path in sorted(nwb_files):
                f.write(file_path + '\n')

rule prepare_data:
    input:
        nwb_file = f"{RAW_DIR}/{DATASET}/{{file}}"
    output:
        temp(f"{PROCESSED_DIR}/{DATASET}/tmp/{{file}}.txt")
    log:
        f".snakemake/logs/{DATASET}/prepare_data.{{file}}.log"
    run:
        from pathlib import Path
        import subprocess

        # Create output directory (including nested structure)
        output_path = Path(output[0])
        output_path.parent.mkdir(parents=True, exist_ok=True)

        # Create log directory
        log_path = Path(log[0])
        log_path.parent.mkdir(parents=True, exist_ok=True)

        # Run prepare_data script
        input_path = Path(input.nwb_file)
        output_dir = Path(f"{PROCESSED_DIR}/{DATASET}")
        cmd = f'python -m brainsets_pipelines.{DATASET}.prepare_data --input_file "{input_path}" --output_dir "{output_dir}"'

        with open(log[0], 'w') as log_file:
            result = subprocess.run(cmd, shell=True, stdout=log_file, stderr=subprocess.STDOUT)
            if result.returncode != 0:
                raise subprocess.CalledProcessError(result.returncode, cmd)

        # Touch output file to indicate completion
        output_path.touch()

def aggregate_input(wildcards):
    with checkpoints.temmar_link_download_data.get(**wildcards).output[0].open() as manifest:
        files = [line.strip() for line in manifest]
    return expand(f"{PROCESSED_DIR}/{DATASET}/tmp/{{file}}.txt", file=files)

rule merge_manifests:
    input:
        aggregate_input
    output:
        f"{PROCESSED_DIR}/{DATASET}/manifest.txt"
    run:
        from pathlib import Path

        # Find all H5 files
        h5_files = []
        proc_dir = Path(f"{PROCESSED_DIR}/{DATASET}")
        for h5_file in proc_dir.rglob("*.h5"):
            rel_path = h5_file.relative_to(proc_dir)
            h5_files.append(str(rel_path).replace("\\", "/"))

        # Write manifest
        with open(output[0], 'w') as f:
            for file_path in sorted(h5_files):
                f.write(file_path + '\n')

rule all:
    input:
        f"{PROCESSED_DIR}/{DATASET}/manifest.txt"
