######################################################
# IBL Reproducible Ephys (2022)
######################################################

try:
    import one
    import brainbox
except:
    raise ImportError("To download IBL data, install ONE-api and brainbox: \n pip install ONE-api \n pip install ibllib")

DATASET = "ibl_reproducible_ephys_2022"
RAW_DIR = config["RAW_DIR"]
PROCESSED_DIR = config["PROCESSED_DIR"]

import os

# Ensure paths are resolved relative to the Snakemake file location
BASE_DIR = os.path.abspath(os.getcwd())
SPLIT_PATH = os.path.join(BASE_DIR, "brainsets_pipelines", DATASET, "splits.npy")

def read_eids(file):
    filepath = os.path.join(BASE_DIR, "brainsets_pipelines", DATASET, file)
    if os.path.exists(filepath):
        with open(filepath) as f:
            return [line.strip() for line in f]
    else:
        print(f"Warning: {file} not found in {filepath}")
        return []

TEST_EIDS = read_eids("test_eids.txt")
TRAIN_EIDS = read_eids("train_eids.txt")

# Rule to prepare data for a single session
rule prepare_data:
    output:
        h5_file = os.path.join(PROCESSED_DIR, DATASET, "{eid}.h5"),
        txt_file = os.path.join(PROCESSED_DIR, DATASET, "tmp", "{eid}.txt")
    log:
        os.path.join(".snakemake", "logs", DATASET, "prepare_data.{eid}.log")
    params:
        eid = "{eid}",
        split_path = SPLIT_PATH,
        output_dir = os.path.join(PROCESSED_DIR, DATASET),
        dataset = DATASET
    shell:
        """
        mkdir -p {params.output_dir}/tmp
        mkdir -p $(dirname {log})
        python -m brainsets_pipelines.{params.dataset}.prepare_data \
            --eid {params.eid} \
            --split_path {params.split_path} \
            --output_dir {params.output_dir} 2>&1 | tee {log}
        echo "{output.h5_file}" > {output.txt_file}
        """

# Function to aggregate input files for merge_manifests
def aggregate_input(wildcards):
    eids = TEST_EIDS + TRAIN_EIDS
    if not eids:
        print("Warning: No EIDs found in test_eids.txt or train_eids.txt")
    return expand(os.path.join(PROCESSED_DIR, DATASET, "tmp", "{eid}.txt"), eid=eids)

# Rule to merge manifests
rule merge_manifests:
    input:
        aggregate_input
    output:
        manifest = os.path.join(PROCESSED_DIR, DATASET, "manifest.txt")
    log:
        os.path.join(".snakemake", "logs", DATASET, "merge_manifests.log")
    params:
        processed_dir = PROCESSED_DIR,  # Pass PROCESSED_DIR as a parameter
        dataset = DATASET,  # Pass DATASET as a parameter
        tmp_dir = os.path.join(PROCESSED_DIR, DATASET, "tmp")  # Path to the tmp directory
    shell:
        """
        # Create the manifest file
        find {params.processed_dir}/{params.dataset}/ -type f -name "*.h5" | sed "s|^{params.processed_dir}/{params.dataset}/||" > {output.manifest}

        # Remove the tmp directory
        if [ -d {params.tmp_dir} ]; then
            echo "Removing tmp directory: {params.tmp_dir}" >> {log}
            rm -rf {params.tmp_dir}
        else
            echo "Warning: tmp directory not found: {params.tmp_dir}" >> {log}
        fi
        """

# Default rule to run the entire workflow
rule all:
    input:
        os.path.join(PROCESSED_DIR, DATASET, "manifest.txt")