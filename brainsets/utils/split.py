import logging
import hashlib
import numpy as np
from typing import List, Dict
from collections import Counter
from temporaldata import Interval, Data


def split_one_epoch(epoch, grid, split_ratios=[0.6, 0.1, 0.3]):
    assert len(epoch) == 1
    epoch_start = epoch.start[0]
    epoch_end = epoch.end[0]

    train_val_split_time = epoch_start + split_ratios[0] * (epoch_end - epoch_start)
    val_test_split_time = train_val_split_time + split_ratios[1] * (
        epoch_end - epoch_start
    )

    grid_match = grid.slice(
        train_val_split_time, train_val_split_time, reset_origin=False
    )
    if len(grid_match) > 0:
        if (
            train_val_split_time - grid_match.start[0]
            > grid_match.end[0] - train_val_split_time
        ):
            train_val_split_time = grid_match.end[0]
        else:
            train_val_split_time = grid_match.start[0]

    grid_match = grid.slice(
        val_test_split_time, val_test_split_time, reset_origin=False
    )
    if len(grid_match) > 0:
        if (
            val_test_split_time - grid_match.start[0]
            > grid_match.end[0] - val_test_split_time
        ):
            val_test_split_time = grid_match.end[0]
        else:
            val_test_split_time = grid_match.start[0]

    train_interval = Interval(start=epoch_start, end=train_val_split_time)
    val_interval = Interval(start=train_interval.end[0], end=val_test_split_time)
    test_interval = Interval(start=val_interval.end[0], end=epoch_end)

    return train_interval, val_interval, test_interval


def split_two_epochs(epoch, grid):
    assert len(epoch) == 2
    first_epoch_start = epoch.start[0]
    first_epoch_end = epoch.end[0]

    split_time = first_epoch_start + 0.5 * (first_epoch_end - first_epoch_start)
    grid_match = grid.slice(split_time, split_time, reset_origin=False)
    if len(grid_match) > 0:
        if split_time - grid_match.start[0] > grid_match.end[0] - split_time:
            split_time = grid_match.end[0]
        else:
            split_time = grid_match.start[0]

    train_interval = Interval(
        start=first_epoch_start,
        end=split_time,
    )
    val_interval = Interval(start=train_interval.end[0], end=first_epoch_end)
    test_interval = epoch.select_by_mask(np.array([False, True]))

    return train_interval, val_interval, test_interval


def split_three_epochs(epoch, grid):
    assert len(epoch) == 3

    test_interval = epoch.select_by_mask(np.array([False, False, True]))
    train_interval = epoch.select_by_mask(np.array([True, True, False]))

    split_time = train_interval.end[1] - 0.3 * (
        train_interval.end[1] - train_interval.start[1]
    )
    grid_match = grid.slice(split_time, split_time, reset_origin=False)
    if len(grid_match) > 0:
        if split_time - grid_match.start[0] > grid_match.end[0] - split_time:
            split_time = grid_match.end[0]
        else:
            split_time = grid_match.start[0]

    train_interval.end[1] = split_time
    val_interval = Interval(start=train_interval.end[1], end=epoch.end[1])

    return train_interval, val_interval, test_interval


def split_four_epochs(epoch, grid):
    assert len(epoch) == 4

    test_interval = epoch.select_by_mask(np.array([False, False, False, True]))
    train_interval = epoch.select_by_mask(np.array([True, True, True, False]))
    split_time = train_interval.end[2] - 0.5 * (
        train_interval.end[2] - train_interval.start[2]
    )
    grid_match = grid.slice(split_time, split_time, reset_origin=False)
    if len(grid_match) > 0:
        if split_time - grid_match.start[0] > grid_match.end[0] - split_time:
            split_time = grid_match.end[0]
        else:
            split_time = grid_match.start[0]

    train_interval.end[2] = split_time
    val_interval = Interval(start=train_interval.end[2], end=epoch.end[2])

    return train_interval, val_interval, test_interval


def split_five_epochs(epoch, grid):
    assert len(epoch) == 5

    train_interval = epoch.select_by_mask(np.array([True, True, True, False, False]))
    test_interval = epoch.select_by_mask(np.array([False, False, False, True, True]))

    split_time = train_interval.end[2] - 0.5 * (
        train_interval.end[2] - train_interval.start[2]
    )
    grid_match = grid.slice(split_time, split_time, reset_origin=False)
    if len(grid_match) > 0:
        if split_time - grid_match.start[0] > grid_match.end[0] - split_time:
            split_time = grid_match.end[0]
        else:
            split_time = grid_match.start[0]

    train_interval.end[2] = split_time
    val_interval = Interval(start=train_interval.end[2], end=epoch.end[2])

    return train_interval, val_interval, test_interval


def split_more_than_five_epochs(epoch):
    assert len(epoch) > 5

    train_interval, val_interval, test_interval = epoch.split(
        [0.6, 0.1, 0.3], shuffle=False
    )
    return train_interval, val_interval, test_interval


def generate_train_valid_test_splits(epoch_dict, grid):
    train_intervals = Interval(np.array([]), np.array([]))
    valid_intervals = Interval(np.array([]), np.array([]))
    test_intervals = Interval(np.array([]), np.array([]))

    for name, epoch in epoch_dict.items():
        if name == "invalid_presentation_epochs":
            logging.warn(f"Found invalid presentation epochs, which will be excluded.")
            continue
        if len(epoch) == 1:
            train, valid, test = split_one_epoch(epoch, grid)
        elif len(epoch) == 2:
            train, valid, test = split_two_epochs(epoch, grid)
        elif len(epoch) == 3:
            train, valid, test = split_three_epochs(epoch, grid)
        elif len(epoch) == 4:
            train, valid, test = split_four_epochs(epoch, grid)
        elif len(epoch) == 5:
            train, valid, test = split_five_epochs(epoch, grid)
        else:
            train, valid, test = split_more_than_five_epochs(epoch)

        train_intervals = train_intervals | train
        valid_intervals = valid_intervals | valid
        test_intervals = test_intervals | test

    return train_intervals, valid_intervals, test_intervals


def chop_intervals(
    intervals: Interval, duration: float, check_no_overlap: bool = False
) -> Interval:
    """
    Subdivides intervals into fixed-length epochs using Interval.arange().

    If some intervals are shorter than the duration, keep them as they are.
    If an interval is not a perfect multiple of the duration, the last chunk will be shorter.

    Args:
        intervals: The original intervals to chop.
        duration: The duration of each chopped interval in seconds.
        check_no_overlap: If True, verify the resulting intervals don't overlap.

    Returns:
        Interval: A new Interval object containing the chopped segments.
                  Metadata from the original intervals is preserved and repeated for each segment.

    Raises:
        ValueError: If check_no_overlap is True and intervals overlap.
    """
    if len(intervals) == 0:
        return Interval(start=np.array([]), end=np.array([]))

    chopped_intervals = []
    original_indices = []

    for i, (start, end) in enumerate(zip(intervals.start, intervals.end)):
        if end - start <= duration:
            chopped = Interval(start=start, end=end)
        else:
            chopped = Interval.arange(start, end, step=duration, include_end=True)

        chopped_intervals.append(chopped)
        original_indices.extend([i] * len(chopped))

    all_starts = np.concatenate([c.start for c in chopped_intervals])
    all_ends = np.concatenate([c.end for c in chopped_intervals])

    kwargs = {}
    if hasattr(intervals, "keys"):
        for key in intervals.keys():
            if key in ["start", "end"]:
                continue
            val = getattr(intervals, key)
            if isinstance(val, np.ndarray) and len(val) == len(intervals):
                kwargs[key] = val[original_indices]

    result = Interval(start=all_starts, end=all_ends, **kwargs)

    if check_no_overlap:
        if not result.is_disjoint():
            raise ValueError("Intervals overlap after chopping")

    return result


def _create_interval_split(intervals: Interval, indices: np.ndarray) -> Interval:
    """Create an Interval subset from indices and sort it."""
    mask = np.zeros(len(intervals), dtype=bool)
    mask[indices] = True
    split = intervals.select_by_mask(mask)
    split.sort()
    return split


def generate_stratified_folds(
    intervals: Interval,
    stratify_by: str,
    n_folds: int = 5,
    val_ratio: float = 0.2,
    seed: int = 42,
) -> List[Data]:
    """
    Generates stratified train/valid/test splits using a two-stage splitting process.

    The splitting is performed in two stages:
        1. Outer split (StratifiedKFold): The intervals are divided into n_folds,
           where each fold uses one partition as the test set and the remaining
           partitions as train+valid. Stratification ensures each fold maintains
           the class distribution of the original data.
        2. Inner split (StratifiedShuffleSplit): The train+valid portion of each fold
           is further split into train and valid sets using val_ratio, while preserving
           the class distribution.

    Args:
        intervals: The intervals to split.
        n_folds: Number of folds for cross-validation.
        val_ratio: Ratio of validation set relative to train+valid combined.
        seed: Random seed.
        stratify_by: The attribute name to use for stratification (e.g., "id", "label",
            "class"). The intervals must have this attribute.

    Returns:
        List of Data objects, one for each fold.

    Raises:
        ValueError: If the intervals don't have the specified stratify_by attribute.
        ValueError: If there are fewer samples than n_folds.
    """
    try:
        from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit
    except ImportError:
        raise ImportError(
            "This function requires the scikit-learn library which you can install with "
            "`pip install scikit-learn`"
        )

    if not hasattr(intervals, stratify_by):
        raise ValueError(
            f"Intervals must have a '{stratify_by}' attribute for stratification."
        )

    class_labels = getattr(intervals, stratify_by)
    if len(class_labels) < n_folds:
        raise ValueError(
            f"Not enough samples ({len(class_labels)}) for {n_folds} folds."
        )

    outer_splitter = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)
    folds = []
    sample_indices = np.arange(len(intervals))

    for fold_idx, (train_val_indices, test_indices) in enumerate(
        outer_splitter.split(sample_indices, class_labels)
    ):
        test_split = _create_interval_split(intervals, test_indices)

        train_val_labels = class_labels[train_val_indices]
        inner_splitter = StratifiedShuffleSplit(
            n_splits=1, test_size=val_ratio, random_state=seed + fold_idx
        )

        for train_indices, val_indices in inner_splitter.split(
            train_val_indices, train_val_labels
        ):
            train_original_indices = train_val_indices[train_indices]
            val_original_indices = train_val_indices[val_indices]

            train_split = _create_interval_split(intervals, train_original_indices)
            val_split = _create_interval_split(intervals, val_original_indices)

            combined_domain = train_split | val_split | test_split

            fold_data = Data(
                train=train_split,
                valid=val_split,
                test=test_split,
                domain=combined_domain,
            )

            folds.append(fold_data)

    return folds


def generate_task_kfold_splits(
    trials: Interval,
    task_configs: Dict[str, List[str]],
    label_field: str,
    n_folds: int = 5,
    val_ratio: float = 0.2,
    seed: int = 42,
) -> Dict[str, Interval]:
    """
    Generate k-fold cross-validation train/valid/test splits for multiple tasks.

    For each task, creates n_folds stratified train/valid/test splits using the
    provided trials Interval object. The splits are returned as Interval objects
    with names formatted as "{task_name}_fold{k}_train", "{task_name}_fold{k}_valid",
    and "{task_name}_fold{k}_test" for each fold.

    Args
    ----
    trials : Interval
        An Interval object containing trial information, including labels.
    task_configs : Dict[str, List[str]]
        Dictionary mapping task names to lists of labels to include for that task.
        Example: {"MotorImagery": ["left_hand", "right_hand", "feet"], ...}
    label_field : str
        The attribute name in trials that contains the labels.
    n_folds : int
        Number of folds for cross-validation. Default is 5.
    val_ratio : float
        Ratio of validation set relative to train+valid combined. Default is 0.2.
    seed : int
        Random seed for reproducibility. Default is 42.

    Returns
    -------
    Dict[str, Interval]
        Dictionary mapping split names to Interval objects.
    """
    if not hasattr(trials, label_field):
        raise ValueError(
            f"Trials must have a '{label_field}' attribute for task filtering."
        )

    all_labels = getattr(trials, label_field)
    splits_dict = {}

    for task_name, include_labels in task_configs.items():
        logging.info(f"\nGenerating {task_name} k-fold train/valid/test splits")

        task_mask = np.isin(all_labels, include_labels)
        task_trials = trials.select_by_mask(task_mask)

        if len(task_trials) < n_folds:
            logging.warning(
                f"Task {task_name} has only {len(task_trials)} trials, "
                f"skipping (need at least {n_folds})"
            )
            continue

        folds = generate_stratified_folds(
            task_trials,
            stratify_by=label_field,
            n_folds=n_folds,
            val_ratio=val_ratio,
            seed=seed,
        )

        for k, fold_data in enumerate(folds):
            task_labels_train = getattr(fold_data.train, label_field)
            task_labels_valid = getattr(fold_data.valid, label_field)
            task_labels_test = getattr(fold_data.test, label_field)

            logging.info(f"Fold {k}:")
            logging.info(f"  Train label counts: {dict(Counter(task_labels_train))}")
            logging.info(f"  Valid label counts: {dict(Counter(task_labels_valid))}")
            logging.info(f"  Test label counts: {dict(Counter(task_labels_test))}")

            splits_dict[f"{task_name}_fold{k}_train"] = fold_data.train
            splits_dict[f"{task_name}_fold{k}_valid"] = fold_data.valid
            splits_dict[f"{task_name}_fold{k}_test"] = fold_data.test

    return splits_dict


def compute_subject_kfold_assignments(
    subject_id: str, n_folds: int = 5, val_ratio: float = 0.2, seed: int = 42
) -> Dict[str, str]:
    """
    Compute deterministic subject-level k-fold train/valid/test assignments.

    Uses hash-based assignment to deterministically assign subjects to folds.
    For each fold k:
    - Subjects in bucket k are assigned to test
    - Remaining subjects are split into train/valid based on val_ratio

    Args
    ----
    subject_id : str
        Subject identifier (e.g., "S001", "subj-001")
    n_folds : int
        Number of folds for cross-validation. Default is 5.
    val_ratio : float
        Ratio of validation set relative to train+valid combined. Default is 0.2.
    seed : int
        Random seed for reproducibility. Default is 42.

    Returns
    -------
    Dict[str, str]
        Dictionary mapping "SubjectSplit_fold{k}" to "train", "valid", or "test"
        for each fold k in range(n_folds).
    """
    subject_str = f"{subject_id}_{seed}"
    subject_bytes = subject_str.encode("utf-8")
    hash_obj = hashlib.md5(subject_bytes)
    hash_int = int(hash_obj.hexdigest(), 16)
    bucket = hash_int % n_folds

    assignments = {}

    for k in range(n_folds):
        if bucket == k:
            assignments[f"SubjectSplit_fold{k}"] = "test"
        else:
            fold_str = f"{subject_id}_{seed}_{k}"
            fold_bytes = fold_str.encode("utf-8")
            fold_hash_obj = hashlib.md5(fold_bytes)
            fold_hash_int = int(fold_hash_obj.hexdigest(), 16)
            normalized_hash = (fold_hash_int % 10000) / 10000.0
            if normalized_hash < val_ratio:
                assignments[f"SubjectSplit_fold{k}"] = "valid"
            else:
                assignments[f"SubjectSplit_fold{k}"] = "train"

    return assignments
